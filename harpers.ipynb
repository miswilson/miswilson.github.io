{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/miawilson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Run this once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'was', 'a', 'decade', 'of', 'fashion', 'iconic', 'styles', 'like', 'flapper']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"The 1920s was a decade of fashion revolutions. Iconic styles like flapper dresses.\"\n",
    "tokens = sample_text.split()  # Split by spaces\n",
    "tokens_lower = [word.lower().strip('.,') for word in tokens if word.isalpha()]\n",
    "print(tokens_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print(*args, sep=' ', end='\\n', file=None, flush=False)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# URL of Harper's Bazaar (example, adjust based on target page)\n",
    "url = \"https://www.harpersbazaar.com/uk/search/?q=1920\"\n",
    "\n",
    "# Fetch page content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all articles or sections mentioning \"1920s\"\n",
    "articles = soup.find_all(string=re.compile(r'1920s', re.IGNORECASE))\n",
    "\n",
    "# Extract surrounding text and metadata\n",
    "data = []\n",
    "for article in articles:\n",
    "    parent = article.parent\n",
    "    context = parent.get_text(strip=True)\n",
    "    data.append({'context': context})\n",
    "\n",
    "# Save data to a CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"1920s_terms.csv\", index=False)\n",
    "print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 1920s_articles_and_terms.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# URL of Harper's Bazaar (adjust to specific section if needed)\n",
    "url = \"https://www.harpersbazaar.com/uk/search/?q=1920\"\n",
    "\n",
    "# Fetch page content\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "else:\n",
    "    print(\"Failed to fetch the webpage:\", response.status_code)\n",
    "    exit()\n",
    "\n",
    "# Define key terms to look for\n",
    "key_terms = [\"flapper\", \"dress\", \"jazz\", \"art deco\", \"modernism\", \"1920s\", \"fashion\"]\n",
    "\n",
    "# Find all articles mentioning \"1920s\"\n",
    "articles = soup.find_all('a', href=True, string=re.compile(r'1920s', re.IGNORECASE))\n",
    "\n",
    "data = []\n",
    "for article in articles:\n",
    "    # Extract article title and link\n",
    "    title = article.get_text(strip=True)\n",
    "    link = article['href']\n",
    "    \n",
    "    # Follow link to extract context (if needed)\n",
    "    if not link.startswith(\"http\"):\n",
    "        link = \"https://www.harpersbazaar.com\" + link  # Adjust to base URL\n",
    "\n",
    "    # Fetch individual article content\n",
    "    article_response = requests.get(link)\n",
    "    if article_response.status_code == 200:\n",
    "        article_soup = BeautifulSoup(article_response.text, 'html.parser')\n",
    "        content = article_soup.get_text(separator=\" \", strip=True)\n",
    "        \n",
    "        # Find key terms in the article content\n",
    "        terms_found = [term for term in key_terms if term in content.lower()]\n",
    "        \n",
    "        # Append data\n",
    "        data.append({'Title': title, 'Link': link, 'Key Terms': \", \".join(terms_found)})\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the data to a CSV\n",
    "df.to_csv(\"1920s_articles_and_terms.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to 1920s_articles_and_terms.csv\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
